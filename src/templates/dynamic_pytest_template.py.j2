"""
Test Suite for {{ api.name }}
Generated by AI API Test Automation Framework
Endpoint: {{ api.method }} {{ api.path }}

Intelligent Test Classification:
- Level: {{ test_level }}
- Markers: {{ test_markers | join(', ') }}
- Estimated Duration: {{ execution_estimate.estimated_duration }}
- Resource Usage: {{ execution_estimate.resource_usage }}
- Dependencies: {{ execution_estimate.dependencies }}

This comprehensive test suite covers:
- Basic functionality testing
- Error scenario validation
- Edge case handling
- Response validation
- Security testing
"""

import pytest
import httpx
import asyncio
import json
from typing import Dict, Any, Optional, List
from datetime import datetime
from unittest.mock import patch, Mock
import structlog

# Configure logging
logger = structlog.get_logger(__name__)

# Test configuration constants
BASE_URL = "{{ base_url | default('http://localhost:8000') }}"
TIMEOUT = {{ timeout | default(30) }}
MAX_RETRIES = {{ max_retries | default(3) }}


class Test{{ api.name | replace(' ', '') | replace('-', '') }}:
    """
    Comprehensive test suite for {{ api.name }} endpoint
    
    Test Coverage:
    - Successful operation scenarios
    - Input validation and error handling
    - Authentication and authorization
    - Rate limiting and performance
    - Edge cases and boundary conditions
    
    Classification: {{ test_markers | join(', ') }}
    """
    
    # Use the global async_client fixture from conftest.py
    # No need to define it here - it will be injected automatically
    
    @pytest.fixture(scope="class")
    def auth_headers(self):
        """Authentication headers for protected endpoints"""
        return {
            "Authorization": "Bearer {{ auth_token }}",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "X-Request-ID": f"test-{datetime.now().isoformat()}",
            "X-Client-Version": "1.0.0"
        }
    
    @pytest.fixture(scope="function")
    def test_payload(self):
        """Valid test payload for the endpoint"""
        return {{ body_params | tojson | safe }}
    
    # ==================== Basic Functionality Tests ====================
    
    @pytest.mark.asyncio
    {% for marker in test_markers -%}
    @pytest.mark.{{ marker }}
    {% endfor -%}
    async def test_{{ api.method.lower() }}_{{ api.name.lower().replace(' ', '_').replace('-', '_') }}_success(
        self, async_client, auth_headers{% if api.method.upper() in ['POST', 'PUT', 'PATCH'] %}, test_payload{% endif %}
    ):
        """
        Test successful {{ api.method }} operation
        
        Validates:
        - Request succeeds with proper data
        - Response has correct status code
        - Response structure matches expectations
        - Required fields are present
        """
        logger.info("Testing basic success case", 
                   method="{{ api.method }}", 
                   path="{{ api.path }}")
        
        {% if api.method.upper() in ['POST', 'PUT', 'PATCH'] -%}
        response = await async_client.{{ api.method.lower() }}(
            "{{ api.path }}",
            headers=auth_headers,
            json=test_payload
        )
        {% else -%}
        response = await async_client.{{ api.method.lower() }}(
            "{{ api.path }}",
            headers=auth_headers
        )
        {% endif -%}
        
        # Basic response validation
        assert response.status_code in [200, 201{% if api.method.upper() == 'DELETE' %}, 204{% endif %}], \
            f"Expected success status, got {response.status_code}: {response.text}"
        
        # Content-type validation for non-empty responses
        if response.status_code != 204 and response.content:
            assert "application/json" in response.headers.get("content-type", ""), \
                f"Expected JSON response, got {response.headers.get('content-type')}"
            
            # Parse JSON response
            response_data = response.json()
            assert response_data is not None, "Response body should not be empty"
            
            # Log response structure for debugging
            logger.info("Response received", 
                       status=response.status_code,
                       data_keys=list(response_data.keys()) if isinstance(response_data, dict) else type(response_data).__name__)
        
        # Performance check for smoke tests
        {% if 'smoke' in test_markers -%}
        response_time = getattr(response, 'elapsed', None)
        if response_time:
            assert response_time.total_seconds() < 5.0, \
                f"Smoke test should complete quickly, took {response_time.total_seconds():.2f}s"
        {% endif -%}
    
    # ==================== Error Handling Tests ====================
    
    {% if 'error_scenarios' in test_markers or 'error_handling' in test_markers -%}
    @pytest.mark.asyncio
    @pytest.mark.error_scenarios
    @pytest.mark.error_handling
    {% if 'smoke' in test_markers -%}
    @pytest.mark.smoke
    {% else -%}
    @pytest.mark.integration
    {% endif -%}
    async def test_{{ api.method.lower() }}_{{ api.name.lower().replace(' ', '_').replace('-', '_') }}_invalid_input(
        self, async_client, auth_headers
    ):
        """
        Test error handling with invalid input
        
        Validates:
        - Invalid requests are rejected appropriately
        - Error responses have proper structure
        - HTTP status codes are correct
        - Error messages are helpful
        """
        {% if api.method.upper() in ['POST', 'PUT', 'PATCH'] -%}
        invalid_payload = {"invalid": "data", "malformed": True}
        
        response = await async_client.{{ api.method.lower() }}(
            "{{ api.path }}",
            headers=auth_headers,
            json=invalid_payload
        )
        
        # Should reject invalid input
        assert response.status_code in [400, 422], \
            f"Expected client error for invalid input, got {response.status_code}"
        {% else -%}
        # For GET/DELETE, test with invalid path parameters or query params
        invalid_path = "{{ api.path }}".replace("{", "invalid_").replace("}", "")
        
        response = await async_client.{{ api.method.lower() }}(
            invalid_path,
            headers=auth_headers
        )
        
        # Should handle invalid resource gracefully
        assert response.status_code in [404, 400], \
            f"Expected not found or bad request, got {response.status_code}"
        {% endif -%}
        
        # Validate error response structure
        if response.content:
            try:
                error_data = response.json()
                assert "error" in error_data or "message" in error_data or "detail" in error_data, \
                    "Error response should contain error information"
            except json.JSONDecodeError:
                # Some APIs return plain text errors, which is acceptable
                pass
    {% endif -%}
    
    # ==================== Authentication Tests ====================
    
    {% if 'auth' in test_markers or 'auth_test' in test_markers -%}
    @pytest.mark.asyncio
    @pytest.mark.auth
    @pytest.mark.auth_test
    {% if 'smoke' in test_markers -%}
    @pytest.mark.smoke
    {% else -%}
    @pytest.mark.integration
    {% endif -%}
    async def test_{{ api.method.lower() }}_{{ api.name.lower().replace(' ', '_').replace('-', '_') }}_unauthorized(
        self, async_client{% if api.method.upper() in ['POST', 'PUT', 'PATCH'] %}, test_payload{% endif %}
    ):
        """
        Test that endpoint requires proper authentication
        
        Validates:
        - Unauthenticated requests are rejected
        - Proper 401 status code is returned
        - Authentication mechanism is enforced
        """
        # No auth headers
        {% if api.method.upper() in ['POST', 'PUT', 'PATCH'] -%}
        response = await async_client.{{ api.method.lower() }}(
            "{{ api.path }}",
            json=test_payload
        )
        {% else -%}
        response = await async_client.{{ api.method.lower() }}(
            "{{ api.path }}"
        )
        {% endif -%}
        
        # Should require authentication
        assert response.status_code == 401, \
            f"Expected 401 Unauthorized, got {response.status_code}: {response.text}"
    {% endif -%}
    
    # ==================== Performance Tests ====================
    
    {% if 'performance' in test_markers or 'load' in test_markers -%}
    @pytest.mark.asyncio
    @pytest.mark.performance
    @pytest.mark.load
    @pytest.mark.slow
    async def test_{{ api.method.lower() }}_{{ api.name.lower().replace(' ', '_').replace('-', '_') }}_performance(
        self, async_client, auth_headers{% if api.method.upper() in ['POST', 'PUT', 'PATCH'] %}, test_payload{% endif %}
    ):
        """
        Test endpoint performance characteristics
        
        Validates:
        - Response times are within acceptable limits
        - Concurrent requests are handled properly
        - Resource usage is reasonable
        """
        import time
        
        # Single request timing
        start_time = time.time()
        
        {% if api.method.upper() in ['POST', 'PUT', 'PATCH'] -%}
        response = await async_client.{{ api.method.lower() }}(
            "{{ api.path }}",
            headers=auth_headers,
            json=test_payload
        )
        {% else -%}
        response = await async_client.{{ api.method.lower() }}(
            "{{ api.path }}",
            headers=auth_headers
        )
        {% endif -%}
        
        execution_time = time.time() - start_time
        
        # Basic success validation
        assert response.status_code < 400, f"Request failed: {response.status_code}"
        
        # Performance validation
        assert execution_time < 10.0, \
            f"Request took too long: {execution_time:.2f}s"
        
        logger.info("Performance test completed", 
                   execution_time=f"{execution_time:.3f}s",
                   status_code=response.status_code)
    {% endif -%}
    
    # ==================== Boundary Tests ====================
    
    {% if 'boundary' in test_markers -%}
    @pytest.mark.asyncio
    @pytest.mark.boundary
    @pytest.mark.integration
    async def test_{{ api.method.lower() }}_{{ api.name.lower().replace(' ', '_').replace('-', '_') }}_boundary_cases(
        self, async_client, auth_headers
    ):
        """
        Test boundary conditions and edge cases
        
        Validates:
        - Large payloads are handled correctly
        - Empty/minimal data is processed properly
        - Edge values don't cause errors
        """
        {% if api.method.upper() in ['POST', 'PUT', 'PATCH'] -%}
        # Test with minimal payload
        minimal_payload = {}
        
        response = await async_client.{{ api.method.lower() }}(
            "{{ api.path }}",
            headers=auth_headers,
            json=minimal_payload
        )
        
        # Should handle minimal input gracefully (may succeed or fail with proper error)
        assert response.status_code < 500, \
            f"Server error with minimal payload: {response.status_code}"
        {% endif -%}
        
        # Additional boundary tests can be added here based on API specifics
        logger.info("Boundary test completed")
    {% endif -%}